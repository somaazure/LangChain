{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "982b31d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7bbf17c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='you are a helpful assistant that gives one liner about the word entered by user', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Sesquipedalian', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chattemplate = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"you are a helpful assistant that gives one liner about the word entered by user\"),\n",
    "        (\"human\",\"{user_input}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "messages = chattemplate.format_messages(user_input = \"Sesquipedalian\")\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b722438a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='you are a helpful assistant that gives one liner about the word entered by user', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='When was it developed', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Albert Einstein developed the theory of relativity.', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='When was it developed', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chattemplate = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"you are a helpful assistant that gives one liner about the word entered by user\"),\n",
    "        (\"human\",\"{user_input}\"),\n",
    "        (\"ai\",\"Albert Einstein developed the theory of relativity.\"),\n",
    "        (\"human\",\"{user_input}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "messages = chattemplate.format_messages(user_input = \"When was it developed\")\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ae0f05fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are a helpful assistant that gives a one-line definition of the word entered by user', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Callous', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import HumanMessagePromptTemplate,SystemMessagePromptTemplate,AIMessagePromptTemplate\n",
    "\n",
    "chattemplate = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessagePromptTemplate.from_template(\"You are a helpful assistant that gives a one-line definition of the word entered by user\"),\n",
    "        HumanMessagePromptTemplate.from_template(\"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "messages = chattemplate.format_messages(input = \"Callous\")\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1e67b818",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "from langchain_core.prompts import FewShotPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8d8a4536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mInit signature:\u001b[39m\n",
      "FewShotPromptTemplate(\n",
      "    *,\n",
      "    name: Optional[str] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    input_variables: list[str],\n",
      "    optional_variables: list[str] = [],\n",
      "    input_types: Dict[str, Any] = <factory>,\n",
      "    output_parser: Optional[langchain_core.output_parsers.base.BaseOutputParser] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    partial_variables: collections.abc.Mapping[str, typing.Any] = <factory>,\n",
      "    metadata: Optional[Dict[str, Any]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    tags: Optional[list[str]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    examples: Optional[list[dict]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    example_selector: Optional[langchain_core.example_selectors.base.BaseExampleSelector] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    validate_template: bool = \u001b[38;5;28;01mFalse\u001b[39;00m,\n",
      "    example_prompt: langchain_core.prompts.prompt.PromptTemplate,\n",
      "    suffix: str,\n",
      "    example_separator: str = \u001b[33m'\\n\\n'\u001b[39m,\n",
      "    prefix: str = \u001b[33m''\u001b[39m,\n",
      "    template_format: Literal[\u001b[33m'f-string'\u001b[39m, \u001b[33m'jinja2'\u001b[39m] = \u001b[33m'f-string'\u001b[39m,\n",
      ") -> \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mDocstring:\u001b[39m      Prompt template that contains few shot examples.\n",
      "\u001b[31mInit docstring:\u001b[39m Initialize the few shot prompt template.\n",
      "\u001b[31mFile:\u001b[39m           c:\\users\\somashekar.inguva\\onedrive - telstra\\documents\\genai_projects\\venv_genai\\lib\\site-packages\\langchain_core\\prompts\\few_shot.py\n",
      "\u001b[31mType:\u001b[39m           ModelMetaclass\n",
      "\u001b[31mSubclasses:\u001b[39m     "
     ]
    }
   ],
   "source": [
    "FewShotPromptTemplate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "517c48fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identify the sentiment of the user review. Here are some examples:\n",
      "\n",
      "User: I love this product\n",
      "AI: positive\n",
      "\n",
      "User: It was an average experience\n",
      "AI: neutral\n",
      "\n",
      "User: I wonder why it is so highly rated.\n",
      "AI: negative\n",
      "\n",
      "user:Well structured. Five stars.\n",
      "AI:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "examples = [\n",
    "    {\n",
    "        \"rev\": \"I love this product\",\n",
    "        \"answer\": \"positive\"\n",
    "    }, {\n",
    "        \"rev\": \"It was an average experience\",\n",
    "        \"answer\": \"neutral\"\n",
    "    },{\n",
    "        \"rev\": \"I wonder why it is so highly rated.\",\n",
    "        \"answer\": \"negative\"\n",
    "    }\n",
    "]\n",
    "\n",
    "example_prompt = PromptTemplate(input_variables = [\"rev\",\"answer\"],\n",
    "                                template = \"\"\"\n",
    "User: {rev}\n",
    "AI: {answer}\"\"\")\n",
    "\n",
    "prefix = \"Identify the sentiment of the user review. Here are some examples:\"\n",
    "\n",
    "suffix = \"\"\"\n",
    "user:{review}\n",
    "AI:\n",
    "\"\"\"\n",
    "\n",
    "few_shot_prompt_template = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=prefix,\n",
    "    suffix=suffix,\n",
    "    input_variables=[\"review\"],\n",
    "    example_separator=\"\\n\"\n",
    ")\n",
    "\n",
    "print(few_shot_prompt_template.format(review=\"Well structured. Five stars.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d8e8ae56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'word': 'Anachronism',\n",
       " 'text': 'Anachronism is something that belongs to a different time period, often appearing out of place in a modern setting.\\n\\nExample: The Shakespearean dialogue in the modern film was seen as an anachronism by many viewers.'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "\n",
    "dict_template = PromptTemplate(\n",
    "    input_variables = [\"word\"],\n",
    "    template = \"Give me a one line definition of {word}. Then, give one example of how it is used.\"\n",
    ")\n",
    "\n",
    "chain = LLMChain(llm = llm, prompt=dict_template)\n",
    "chain.invoke(\"Anachronism\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ababb7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "ip_list= [\n",
    "    {\"word\": \"Sesquipedalian\"},\n",
    "    {\"word\": \"Anachronism\"},\n",
    "    {\"word\": \"Onomatopoeia\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fd6f46cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': \"Sesquipedalian: characterized by long words; polysyllabic or complex language.\\n\\nExample: The speaker's sesquipedalian speech left many of the audience members confused and unable to follow along.\"},\n",
       " {'text': 'Anachronism is something or someone that is not in its correct historical or chronological time period. \\n\\nExample: In the movie Gladiator, the character Maximus wears a wristwatch, which is an anachronism since wristwatches were not invented or worn during ancient Roman times.'},\n",
       " {'text': 'Onomatopoeia is a figure of speech where words imitate the sound they represent, such as \"buzz\" or \"splash.\" \\n\\nExample: The bees buzzed around the hive in a constant hum.'}]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.apply(input_list=ip_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "598df65e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LLMResult(generations=[[ChatGeneration(text=\"Sesquipedalian means using long words or a long-winded style of speech or writing.\\n\\nExample sentence: The professor's sesquipedalian lecture left the students feeling overwhelmed and confused.\", generation_info={'finish_reason': 'stop', 'logprobs': None}, message=AIMessage(content=\"Sesquipedalian means using long words or a long-winded style of speech or writing.\\n\\nExample sentence: The professor's sesquipedalian lecture left the students feeling overwhelmed and confused.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 40, 'prompt_tokens': 30, 'total_tokens': 70, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BW4A52AbMuMc979ju7KmblLbM6mo5', 'finish_reason': 'stop', 'logprobs': None}, id='run-f3e68c5b-d773-4ef4-ac45-2704326b068a-0', usage_metadata={'input_tokens': 30, 'output_tokens': 40, 'total_tokens': 70, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}))], [ChatGeneration(text='Anachronism is the placement of something in a time period where it does not belong.\\n\\nExample: In the movie \"Gladiator,\" a character is seen wearing a wristwatch, which is an anachronism as wristwatches were not invented until centuries after the film\\'s setting.', generation_info={'finish_reason': 'stop', 'logprobs': None}, message=AIMessage(content='Anachronism is the placement of something in a time period where it does not belong.\\n\\nExample: In the movie \"Gladiator,\" a character is seen wearing a wristwatch, which is an anachronism as wristwatches were not invented until centuries after the film\\'s setting.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 60, 'prompt_tokens': 30, 'total_tokens': 90, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BW4A6qb9fb1qMXdN7o8TNEnL3Zj9p', 'finish_reason': 'stop', 'logprobs': None}, id='run-c02b8bbf-0be4-4b7a-934f-0e4a0b70baa7-0', usage_metadata={'input_tokens': 30, 'output_tokens': 60, 'total_tokens': 90, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}))], [ChatGeneration(text='Onomatopoeia is the formation of a word from a sound associated with what is named (e.g., cuckoo or sizzle).\\n\\nExample: The bees buzzed around the flowers in the garden.', generation_info={'finish_reason': 'stop', 'logprobs': None}, message=AIMessage(content='Onomatopoeia is the formation of a word from a sound associated with what is named (e.g., cuckoo or sizzle).\\n\\nExample: The bees buzzed around the flowers in the garden.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 43, 'prompt_tokens': 31, 'total_tokens': 74, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BW4A79AQrk3puuxe4EWibhFVtLRk8', 'finish_reason': 'stop', 'logprobs': None}, id='run-0814622e-86ae-4f3c-96fe-cb64b41ebefe-0', usage_metadata={'input_tokens': 31, 'output_tokens': 43, 'total_tokens': 74, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}))]], llm_output={'token_usage': {'completion_tokens': 143, 'prompt_tokens': 91, 'total_tokens': 234, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo'}, run=[RunInfo(run_id=UUID('f3e68c5b-d773-4ef4-ac45-2704326b068a')), RunInfo(run_id=UUID('c02b8bbf-0be4-4b7a-934f-0e4a0b70baa7')), RunInfo(run_id=UUID('0814622e-86ae-4f3c-96fe-cb64b41ebefe'))], type='LLMResult')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.generate(ip_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3f08c35d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numexpr\n",
      "  Downloading numexpr-2.10.2-cp311-cp311-win_amd64.whl (144 kB)\n",
      "     -------------------------------------- 145.0/145.0 kB 9.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.23.0 in c:\\users\\somashekar.inguva\\onedrive - telstra\\documents\\genai_projects\\venv_genai\\lib\\site-packages (from numexpr) (1.26.4)\n",
      "Installing collected packages: numexpr\n",
      "Successfully installed numexpr-2.10.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install numexpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6600f587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMMathChain chain...\u001b[0m\n",
      "Calculate (625 raised to power (0.225))-(log10(100))\u001b[32;1m\u001b[1;3m```text\n",
      "625**0.225 - log10(100)\n",
      "```\n",
      "...numexpr.evaluate(\"625**0.225 - log10(100)\")...\n",
      "\u001b[0m\n",
      "Answer: \u001b[33;1m\u001b[1;3m2.2566996126039234\u001b[0m\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'Calculate (625 raised to power (0.225))-(log10(100))',\n",
       " 'answer': 'Answer: 2.2566996126039234'}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import LLMMathChain\n",
    "# from numpy import numexpr\n",
    "\n",
    "calc = LLMMathChain.from_llm(llm,verbose=True)\n",
    "calc.invoke(\"Calculate (625 raised to power (0.225))-(log10(100))\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "67434342",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translate a math problem into a expression that can be executed using Python's numexpr library. Use the output of running this code to answer the question.\n",
      "\n",
      "Question: ${{Question with math problem.}}\n",
      "```text\n",
      "${{single line mathematical expression that solves the problem}}\n",
      "```\n",
      "...numexpr.evaluate(text)...\n",
      "```output\n",
      "${{Output of running the code}}\n",
      "```\n",
      "Answer: ${{Answer}}\n",
      "\n",
      "Begin.\n",
      "\n",
      "Question: What is 37593 * 67?\n",
      "```text\n",
      "37593 * 67\n",
      "```\n",
      "...numexpr.evaluate(\"37593 * 67\")...\n",
      "```output\n",
      "2518731\n",
      "```\n",
      "Answer: 2518731\n",
      "\n",
      "Question: 37593^(1/5)\n",
      "```text\n",
      "37593**(1/5)\n",
      "```\n",
      "...numexpr.evaluate(\"37593**(1/5)\")...\n",
      "```output\n",
      "8.222831614237718\n",
      "```\n",
      "Answer: 8.222831614237718\n",
      "\n",
      "Question: {question}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(calc.prompt.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "19d1a762",
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_llm = ChatOpenAI()\n",
    "msg = [\n",
    "    (\"human\", \"Calculate (625 raised to power (0.225))-(log10(100))\")\n",
    "]\n",
    "\n",
    "calc = calc_llm.invoke(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8673da72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(625^0.225) - log10(100)\n",
      "= 5 - 2\n",
      "= 3\n"
     ]
    }
   ],
   "source": [
    "print(calc.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9e9c2642",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ad586bb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'AeroGlow Nightlight',\n",
       " 'description': 'AeroGlow Nightlight is a smart, voice-activated nightlight that projects calming, animated constellations onto your ceiling.',\n",
       " 'copy_text': '\"Transform your bedtime routine with AeroGlow Nightlight - the voice-activated nightlight that brings the wonders of the cosmos to your ceiling.\"',\n",
       " 'translated_copy': '\"ఏరోగ్లో నైట్లైట్ - దీవానికి ప్రతిస్పందించే గతివిధానం తో మీ గుండతో సృష్టికి ఆశ్చర్యాన్ని తీసుకొస్తుంది, మురికి కల్పనలు తీసుకొస్తుంది.\"'}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_llm = ChatOpenAI()\n",
    "marketing_text = PromptTemplate(\n",
    "    input_variables =['name','description'],\n",
    "    template = \"Generate a one line facebook ad copy for a product called {name}. Below is a description of this product: {description}\"\n",
    ")\n",
    "text_chain = LLMChain(llm=text_llm, prompt = marketing_text,output_key = \"copy_text\")\n",
    "\n",
    "trans_llm = ChatOpenAI()\n",
    "Trans_text = translate_text = PromptTemplate(input_variables=[\"copy_text\"], template=\"\"\"Translate this text to Telugu:\n",
    "{copy_text} \"\"\")\n",
    "translate_chain = LLMChain(llm=trans_llm, prompt = Trans_text,output_key = \"translated_copy\")\n",
    "\n",
    "from langchain.chains import SequentialChain\n",
    "\n",
    "seq_chain = SequentialChain(\n",
    "    chains = [text_chain,translate_chain],\n",
    "    input_variables = [\"name\",\"description\"],\n",
    "    output_variables = [\"copy_text\",\"translated_copy\"]\n",
    ")\n",
    "\n",
    "seq_chain({\"name\":\"AeroGlow Nightlight\",\"description\":\"AeroGlow Nightlight is a smart, voice-activated nightlight that projects calming, animated constellations onto your ceiling.\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7ac61f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f80fb4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI()\n",
    "\n",
    "Dict_template = PromptTemplate(\n",
    "    input_variables = [\"word\"],\n",
    "    template = \"Give me a one line definition of {word}. Response should not contain the word - {word} itself, only the meaning.\"\n",
    ")\n",
    "\n",
    "chain = Dict_template | llm |StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c212db88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A word that sounds like the noise it is describing.'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"Onomatopoeia\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "09019bb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Buzz'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gey_temp = PromptTemplate(\n",
    "    input_variables = [\"definition\"],\n",
    "    template = \"Give me an English word which has the following meaning- {definition}.\"\n",
    ")\n",
    "chain2 = gey_temp.pipe(llm).pipe(StrOutputParser())\n",
    "chain2.invoke(\"A word that sounds like the noise it is describing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b559e680",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Grandiloquent'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_chain = {\"definition\":chain} | chain2\n",
    "seq_chain.invoke({\"word\":\"Sesquipedalian\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5e5e6131",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough, RunnableParallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "7cfcfdb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'somashekar'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = RunnablePassthrough()\n",
    "\n",
    "chain.invoke(\"somashekar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "cc24e61b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def oplen(input:str):\n",
    "    return len(input)\n",
    "\n",
    "chain = RunnableLambda(oplen)\n",
    "chain.invoke(\"somashekar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "32cbe097",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'somashekarInguva', 'length': 16}"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = RunnableParallel(text = RunnablePassthrough(),length = RunnableLambda(oplen))\n",
    "chain.invoke(\"somashekarInguva\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "19e7fc2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Package(s) not found: groq\n"
     ]
    }
   ],
   "source": [
    "!pip show groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "36b43fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "346c9240",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "def message_gemini(prompt):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "      ]\n",
    "    gemini = google.generativeai.GenerativeModel(\n",
    "    model_name='gemini-1.5-flash',\n",
    "    system_instruction=system_message\n",
    ")\n",
    "    response = gemini.generate_content(prompt)\n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "83ce76f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Today is October 26, 2023.\\n'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system_message = \"You are a helpful assistant\"\n",
    "message_gemini(\"What is today's date?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "251e5685",
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer_template = PromptTemplate(\n",
    "    input_variables =[\"topic\"],\n",
    "    template = \"Explain this topic in a single paragraph as if you were explaining it to a 10 year old - {topic}.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "a89c1526",
   "metadata": {},
   "outputs": [],
   "source": [
    "gapi_key = os.getenv('GOOGLE_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "c4123c7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Imagine the Earth is a giant magnet, but instead of attracting metal, it attracts *everything*! That's gravity!  It's an invisible force that pulls everything towards the center of the Earth, which is why we don't float away.  The bigger something is, the stronger its gravity.  The sun's gravity is so strong it keeps all the planets spinning around it!  Even you have gravity – it's just too weak to notice because you're so small, but it's there, pulling tiny things towards you!\""
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "googlellm = ChatGoogleGenerativeAI(model = \"gemini-1.5-flash\",api_key=gapi_key)\n",
    "\n",
    "google_chain = explainer_template | googlellm | StrOutputParser()\n",
    "google_chain.invoke({\"topic\": \"Gravity\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "8ffd05aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Gravity is the invisible force that pulls things towards each other, like how the Earth pulls us down so we don’t float away into space. It’s what keeps us grounded and makes things fall to the ground when we drop them. Gravity is why we stay on the Earth instead of floating away into the sky!'"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "explainer_template = PromptTemplate(\n",
    "    input_variables =[\"topic\"],\n",
    "    template = \"Explain this topic in a single paragraph as if you were explaining it to a 10 year old - {topic}.\"\n",
    ")\n",
    "\n",
    "openAIllm = ChatOpenAI()\n",
    "\n",
    "OpenAI_chain = explainer_template | openAIllm | StrOutputParser()\n",
    "OpenAI_chain.invoke({\"topic\": \"Gravity\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "8425e349",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Explanation 2 is better for explaining gravity to 10 year old students. It uses vivid imagery and relatable examples to help students understand the concept in a more engaging and tangible way. It also includes additional information about how gravity works on a larger scale, which can help expand the students' understanding of the topic.\""
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyzer_template = PromptTemplate(\n",
    "    input_variables =[\"topic\", \"explanation1\", \"explanation2\"],\n",
    "    template = \"\"\"\n",
    "    Which of the two explanations given below are better for explaining {topic} to 10 year old students.\n",
    "    ###\n",
    "    Explanation 1 - {explanation1}\n",
    "    ###\n",
    "    ***\n",
    "    Explanation 2 - {explanation2}\n",
    "    ***\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "combined_chain = (\n",
    "    RunnableParallel({\"explanation1\": OpenAI_chain, \"explanation2\": google_chain, \"topic\": RunnablePassthrough()})\n",
    "    | analyzer_template | openAIllm | StrOutputParser()\n",
    ")\n",
    "\n",
    "combined_chain.invoke({\"topic\": \"Gravity\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "425826f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A system for representing and processing human language through a sequence of interconnected nodes or units.'"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "\n",
    "dict_chattemp = PromptTemplate(\n",
    "    input_variables = [\"word\"],\n",
    "    template = \"Give me a one line definition of {word}. Response should not contain the word itself, only the meaning.\"\n",
    ")\n",
    "\n",
    "chain = dict_chattemp | llm | StrOutputParser()\n",
    "chain.invoke(\"langchain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "12da9fec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"word\": string  // word entered by user\n",
      "\t\"meaning\": string  // One line meaning of the word given by user\n",
      "\t\"example\": string  // An example of how that word can be used in a line\n",
      "\t\"Etymology\": string  // Origin or history of the word\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "from langchain.output_parsers import StructuredOutputParser,ResponseSchema\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "response_schemas = [\n",
    "    ResponseSchema(name=\"word\",description=\"word entered by user\"),\n",
    "    ResponseSchema(name=\"meaning\", description=\"One line meaning of the word given by user\"),\n",
    "    ResponseSchema(name=\"example\", description=\"An example of how that word can be used in a line\"),\n",
    "    ResponseSchema(name=\"Etymology\", description=\"Origin or history of the word\")\n",
    "]\n",
    "\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "format_inst = output_parser.get_format_instructions()\n",
    "print(format_inst)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "9019c7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Prompt = PromptTemplate(\n",
    "    template = \"Provide the meaning, an example of how the word is used in a sentence and the etymology of this word: {word}.\\n{format_inst}\",\n",
    "    input_variables = [\"word\"],\n",
    "    partial_variables = {\"format_inst\":format_inst}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "8fb251cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'word': 'Onomatopoeia',\n",
       " 'meaning': 'The formation of a word from a sound associated with what is named',\n",
       " 'example': \"The word 'buzz' is an example of onomatopoeia because it imitates the sound made by a bee.\",\n",
       " 'Etymology': \"The word 'onomatopoeia' comes from the Greek words 'onoma' meaning name and 'poiein' meaning to make. It was first used in the English language in the mid-16th century.\"}"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = Prompt | llm | output_parser\n",
    "chain.invoke(\"Onomatopoeia\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "cb716b0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sachin Tendulkar',\n",
       " 'Virat Kohli',\n",
       " 'Ricky Ponting',\n",
       " 'Brian Lara',\n",
       " 'Jacques Kallis']"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
    "\n",
    "output_parser = CommaSeparatedListOutputParser()\n",
    "format_inst = output_parser.get_format_instructions()\n",
    "\n",
    "Prompt = PromptTemplate(\n",
    "    template = \"list top five {topic}.\\n {format_inst}\",\n",
    "    input_variables = [\"topic\"],\n",
    "    partial_variables = {\"format_inst\":format_inst}\n",
    ")\n",
    "\n",
    "chain = Prompt | llm | output_parser\n",
    "chain.invoke({\"topic\":\"cricketors\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba27fe94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1960-07-01 00:00:00\n"
     ]
    }
   ],
   "source": [
    "from langchain.output_parsers import DatetimeOutputParser\n",
    "\n",
    "output_parser = DatetimeOutputParser()\n",
    "\n",
    "template = \"\"\"Answer the following  Question:\n",
    "{question}\n",
    "\n",
    "{format_instructions}\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(\n",
    "    template = template,\n",
    "    partial_variables = {\"format_instructions\": output_parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "chain = prompt | llm | output_parser \n",
    "# chain = LLMChain(prompt=prompt,llm=llm,output_parser=output_parser, memory=)\n",
    "output = chain.invoke(\"when did somalia got independence\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "4659df13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI, OpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.memory import ConversationBufferMemory "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "c515ddee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\somashekar.inguva\\AppData\\Local\\Temp\\ipykernel_34252\\1550412662.py:8: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory()\n"
     ]
    }
   ],
   "source": [
    "Prompt_Template = PromptTemplate(\n",
    "    input_variables = [\"history\",\"new_input\"],\n",
    "    template = \"\"\" you are having a chat with human\n",
    "    previouschat :{history}\n",
    "    Human:{new_input}\n",
    "    Assistant:\"\"\"\n",
    ")\n",
    "memory = ConversationBufferMemory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "ba4f1a10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are having a chat with a human.\n",
      "    previous chat: []\n",
      "    Human: Which country is the biggest exporter of cotton?\n",
      "    Assistant: \u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'new_input': 'Which country is the biggest exporter of cotton?',\n",
       " 'history': [HumanMessage(content='Which country is the biggest exporter of cotton?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='China', additional_kwargs={}, response_metadata={})],\n",
       " 'text': 'China'}"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import PromptTemplate\n",
    "from langchain_openai import OpenAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.memory import ConversationBufferMemory #This stores the entire conversation history in memory without any additional processing.\n",
    "\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables =[\"history\", \"new_input\"],\n",
    "    template = \"\"\"You are having a chat with a human.\n",
    "    previous chat: {history}\n",
    "    Human: {new_input}\n",
    "    Assistant: \"\"\"\n",
    ")\n",
    "\n",
    "memory = ConversationBufferMemory(return_messages=True)\n",
    "\n",
    "#llm = ChatOpenAI()\n",
    "llm = OpenAI()\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt_template, memory = memory, verbose=True)\n",
    "\n",
    "chain.invoke({\"new_input\": \"Which country is the biggest exporter of cotton?\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "20d19689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are having a chat with a human.\n",
      "    previous chat: Human: Which country is the biggest exporter of cotton?\n",
      "AI:  The biggest exporter of cotton is currently India.\n",
      "    Human: who is second\n",
      "    Assistant: \u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'new_input': 'who is second',\n",
       " 'history': 'Human: Which country is the biggest exporter of cotton?\\nAI:  The biggest exporter of cotton is currently India.',\n",
       " 'text': 'China is currently the second biggest exporter of cotton.'}"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"new_input\":\"who is second\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "c86c13f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import SystemMessage\n",
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    SystemMessagePromptTemplate,MessagesPlaceholder\n",
    ")\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "8531cfd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: You are having a chat with a human.\n",
      "Human: Which country is the biggest exporter of cotton?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'new_input': 'Which country is the biggest exporter of cotton?',\n",
       " 'history': [HumanMessage(content='Which country is the biggest exporter of cotton?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='The biggest exporter of cotton in the world is the United States. Would you like to know more about cotton production and its significance in global trade?', additional_kwargs={}, response_metadata={})],\n",
       " 'text': 'The biggest exporter of cotton in the world is the United States. Would you like to know more about cotton production and its significance in global trade?'}"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Chat_Prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(\n",
    "            content=\"You are having a chat with a human.\"\n",
    "        ),\n",
    "        MessagesPlaceholder(\n",
    "            variable_name=\"history\"\n",
    "        ),\n",
    "        HumanMessagePromptTemplate.from_template(\"{new_input}\")\n",
    "    ]\n",
    ")\n",
    "chat_llm = ChatOpenAI()\n",
    "\n",
    "chat_memory = ConversationBufferMemory(return_messages=True)\n",
    "\n",
    "chat_chain = LLMChain(llm = chat_llm,prompt = Chat_Prompt ,memory=chat_memory ,verbose = True )\n",
    "chat_chain.invoke({\"new_input\":\"Which country is the biggest exporter of cotton?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "4d96ff82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: You are having a chat with a human.\n",
      "Human: Which country is the biggest exporter of cotton?\n",
      "AI: The biggest exporter of cotton is the United States. It is one of the largest cotton-producing countries in the world and exports significant amounts of cotton to various countries.\n",
      "Human: who is second\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'new_input': 'who is second',\n",
       " 'history': [HumanMessage(content='Which country is the biggest exporter of cotton?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='The biggest exporter of cotton is the United States. It is one of the largest cotton-producing countries in the world and exports significant amounts of cotton to various countries.', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='who is second', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='The second largest exporter of cotton is India. India is also a major producer and exporter of cotton, contributing significantly to the global cotton market.', additional_kwargs={}, response_metadata={})],\n",
       " 'text': 'The second largest exporter of cotton is India. India is also a major producer and exporter of cotton, contributing significantly to the global cotton market.'}"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_chain.invoke({\"new_input\":\"who is second\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31fc94e",
   "metadata": {},
   "source": [
    "## SelfNOtes:\n",
    "The difference in responses between `ChatOpenAI()` and `OpenAI()` arises due to how they interact with models and format prompts:\n",
    "\n",
    "1. `ChatOpenAI()` is designed for **chat-based models** (like GPT-4-turbo) and handles multi-turn conversations with system, user, and assistant roles.\n",
    "2. `OpenAI()` is for **completion-style models**, which take plain text input and return a continuation, not structured messages.\n",
    "3. Chat models interpret instructions (like system prompts) more effectively due to role-based formatting.\n",
    "4. `ChatPromptTemplate` fits naturally with `ChatOpenAI()` but not with `OpenAI()`, causing behavioral differences.\n",
    "5. Hence, the same input can yield different answers due to formatting, prompt interpretation, and model design.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a1bacd",
   "metadata": {},
   "source": [
    "For **better, more controllable, and natural results in conversation-like scenarios**, you should use **`ChatOpenAI()`**. Here's why:\n",
    "\n",
    "1. **Designed for chat models** like GPT-3.5-turbo and GPT-4-turbo, which are more advanced for dialogue and context handling.\n",
    "2. **Supports roles** (system/user/assistant), letting you guide the model’s behavior more precisely.\n",
    "3. Handles **multi-turn memory and context** better with tools like `ConversationBufferMemory`.\n",
    "4. Is more aligned with how OpenAI currently fine-tunes and deploys their best models.\n",
    "5. `OpenAI()` is better only for legacy use cases or pure text completions.\n",
    "\n",
    "**Recommendation:** Use `ChatOpenAI()` for modern, high-quality results in chat, agents, or interactive apps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "2d86eb4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: You are having a chat with a human.\n",
      "Human: Which country is the biggest exporter of Milk?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'new_input': 'Which country is the biggest exporter of Milk?',\n",
       " 'prev_msg': [HumanMessage(content='Which country is the biggest exporter of Milk?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='The biggest exporter of milk in the world is New Zealand.', additional_kwargs={}, response_metadata={})],\n",
       " 'text': 'The biggest exporter of milk in the world is New Zealand.'}"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Chat_Prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(\n",
    "            content=\"You are having a chat with a human.\"\n",
    "        ),\n",
    "        MessagesPlaceholder(\n",
    "            variable_name=\"prev_msg\"\n",
    "        ),\n",
    "        HumanMessagePromptTemplate.from_template(\"{new_input}\")\n",
    "    ]\n",
    ")\n",
    "chat_llm = ChatOpenAI()\n",
    "\n",
    "chat_memory = ConversationBufferMemory(memory_key=\"prev_msg\",return_messages=True)\n",
    "\n",
    "chat_chain = LLMChain(llm = chat_llm,prompt = Chat_Prompt ,memory=chat_memory ,verbose = True )\n",
    "chat_chain.invoke({\"new_input\":\"Which country is the biggest exporter of Milk?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "0f34d6c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: You are having a chat with a human.\n",
      "Human: Which country is the biggest exporter of Milk?\n",
      "AI: As of 2021, the biggest exporter of milk is New Zealand. It is followed by the European Union countries and the United States.\n",
      "Human: Which country is the 2nd biggest exporter of Milk?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'new_input': 'Which country is the 2nd biggest exporter of Milk?',\n",
       " 'prev_msg': [HumanMessage(content='Which country is the biggest exporter of Milk?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='As of 2021, the biggest exporter of milk is New Zealand. It is followed by the European Union countries and the United States.', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Which country is the 2nd biggest exporter of Milk?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='The European Union countries as a whole are often considered the second-largest exporter of milk and dairy products in the world.', additional_kwargs={}, response_metadata={})],\n",
       " 'text': 'The European Union countries as a whole are often considered the second-largest exporter of milk and dairy products in the world.'}"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_chain.invoke({\"new_input\":\"Which country is the 2nd biggest exporter of Milk?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "9a0b76f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': ''}"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_memory = ConversationBufferMemory(memory_key=\"history\")\n",
    "sample_memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "638c5cca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'Human: which cntry has more cars\\nAI: usa'}"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_memory.chat_memory.add_user_message(\"which cntry has more cars\")\n",
    "sample_memory.chat_memory.add_ai_message(\"usa\")\n",
    "sample_memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "c82ce9f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are having a chat with a human.\n",
      "    previous chat: Human: which cntry has more cars\n",
      "AI: usa\n",
      "Human: which cntry has more cars\n",
      "AI: China has more cars than any other country in the world.\n",
      "Human: whats the percentage share it has\n",
      "AI: China has approximately 23% of the world's total number of cars.\n",
      "Human: whats the percentage share it has\n",
      "AI: China has approximately 23% of the world's total number of cars.\n",
      "    Human: whats the percentage share it has\n",
      "    Assistant: \u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'new_input': 'whats the percentage share it has',\n",
       " 'history': \"Human: which cntry has more cars\\nAI: usa\\nHuman: which cntry has more cars\\nAI: China has more cars than any other country in the world.\\nHuman: whats the percentage share it has\\nAI: China has approximately 23% of the world's total number of cars.\\nHuman: whats the percentage share it has\\nAI: China has approximately 23% of the world's total number of cars.\",\n",
       " 'text': \"I apologize for the repetition. As I mentioned before, China has approximately 23% of the world's total number of cars.\"}"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import PromptTemplate\n",
    "from langchain_openai import OpenAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables =[\"history\", \"new_input\"],\n",
    "    template = \"\"\"You are having a chat with a human.\n",
    "    previous chat: {history}\n",
    "    Human: {new_input}\n",
    "    Assistant: \"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "\n",
    "# chat_memory = ConversationBufferMemory(memory_key=\"history\",return_messages=True)\n",
    "\n",
    "chat_chain = LLMChain(llm = llm,prompt = prompt_template ,memory=sample_memory ,verbose = True )\n",
    "chat_chain.invoke({\"new_input\":\"whats the percentage share it has\"})\n",
    "# chat_chain.invoke()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "3a779f63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: which contry is the largest producer of cars\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'which contry is the largest producer of cars',\n",
       " 'history': '',\n",
       " 'response': \"The largest producer of cars is currently China, followed by the United States, Japan, Germany, and India. China has the largest number of automobile manufacturers and produces the most cars globally. Japan is known for its high-quality vehicles and innovative technology. Germany is home to some of the world's most prestigious car brands like Mercedes-Benz, BMW, and Volkswagen. India has a growing automotive industry, with companies like Tata Motors and Maruti Suzuki leading the way.\"}"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import PromptTemplate\n",
    "from langchain_openai import OpenAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationChain\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key=\"history\")\n",
    "\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "\n",
    "# chat_memory = ConversationBufferMemory(memory_key=\"history\",return_messages=True)\n",
    "\n",
    "chat_chain = ConversationChain(llm = llm,memory=memory ,verbose = True )\n",
    "chat_chain.invoke({\"input\":\"which contry is the largest producer of cars\"})\n",
    "# chat_chain.invoke()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "8283f65b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: which contry is the largest producer of cars\n",
      "AI: The largest producer of cars is currently China, followed by the United States, Japan, Germany, and India. China has the largest number of automobile manufacturers and produces the most cars globally. Japan is known for its high-quality vehicles and innovative technology. Germany is home to some of the world's most prestigious car brands like Mercedes-Benz, BMW, and Volkswagen. India has a growing automotive industry, with companies like Tata Motors and Maruti Suzuki leading the way.\n",
      "Human: whats the percentage of cars\n",
      "AI:  I'm sorry, could you please clarify what you mean by \"percentage of cars\"? Are you referring to market share, production volume, or something else?\n",
      "Human: whats the percentage of marketshare\n",
      "AI: As of 2020, China holds the largest market share in the global automotive industry at around 30%, followed by the United States with approximately 20%, Japan with around 12%, Germany with about 5%, and India with around 4%. These percentages can vary each year based on production levels and consumer demand.\n",
      "Human: whats the percentage of marketshare of cars\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'whats the percentage of marketshare of cars',\n",
       " 'history': 'Human: which contry is the largest producer of cars\\nAI: The largest producer of cars is currently China, followed by the United States, Japan, Germany, and India. China has the largest number of automobile manufacturers and produces the most cars globally. Japan is known for its high-quality vehicles and innovative technology. Germany is home to some of the world\\'s most prestigious car brands like Mercedes-Benz, BMW, and Volkswagen. India has a growing automotive industry, with companies like Tata Motors and Maruti Suzuki leading the way.\\nHuman: whats the percentage of cars\\nAI:  I\\'m sorry, could you please clarify what you mean by \"percentage of cars\"? Are you referring to market share, production volume, or something else?\\nHuman: whats the percentage of marketshare\\nAI: As of 2020, China holds the largest market share in the global automotive industry at around 30%, followed by the United States with approximately 20%, Japan with around 12%, Germany with about 5%, and India with around 4%. These percentages can vary each year based on production levels and consumer demand.',\n",
       " 'response': \"I'm sorry, I do not have specific data on the percentage of market share for individual countries in terms of cars specifically. However, based on the overall market share information I provided earlier, China is the largest producer of cars and likely holds a significant portion of the global market share for cars. Other countries like the United States, Japan, Germany, and India also contribute to the overall market share of cars.\"}"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_chain.invoke({\"input\":\"whats the percentage of marketshare of cars\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "9e1dac0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "{history}\n",
      "Human: {input}\n",
      "AI:\n"
     ]
    }
   ],
   "source": [
    "print(chat_chain.prompt.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "b6a763ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: which contry is the largest producer of cars\n",
      "AI: The largest producer of cars is currently China, followed by the United States, Japan, Germany, and India. China has the largest number of automobile manufacturers and produces the most cars globally. Japan is known for its high-quality vehicles and innovative technology. Germany is home to some of the world's most prestigious car brands like Mercedes-Benz, BMW, and Volkswagen. India has a growing automotive industry, with companies like Tata Motors and Maruti Suzuki leading the way.\n",
      "Human: whats the percentage of cars\n",
      "AI:  I'm sorry, could you please clarify what you mean by \"percentage of cars\"? Are you referring to market share, production volume, or something else?\n",
      "Human: whats the percentage of marketshare\n",
      "AI: As of 2020, China holds the largest market share in the global automotive industry at around 30%, followed by the United States with approximately 20%, Japan with around 12%, Germany with about 5%, and India with around 4%. These percentages can vary each year based on production levels and consumer demand.\n",
      "Human: whats the percentage of marketshare of cars\n",
      "AI: I'm sorry, I do not have specific data on the percentage of market share for individual countries in terms of cars specifically. However, based on the overall market share information I provided earlier, China is the largest producer of cars and likely holds a significant portion of the global market share for cars. Other countries like the United States, Japan, Germany, and India also contribute to the overall market share of cars.\n"
     ]
    }
   ],
   "source": [
    "print(chat_chain.memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "350fe8f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: which contry is the largest producer of cars\n",
      "AI: The largest producer of cars is currently China, followed by the United States, Japan, Germany, and India. China has the largest number of automobile manufacturers and produces the most cars globally. Japan is known for its high-quality vehicles and innovative technology. Germany is home to some of the world's most prestigious car brands like Mercedes-Benz, BMW, and Volkswagen. India has a growing automotive industry, with companies like Tata Motors and Maruti Suzuki leading the way.\n",
      "Human: whats the percentage of cars\n",
      "AI:  I'm sorry, could you please clarify what you mean by \"percentage of cars\"? Are you referring to market share, production volume, or something else?\n",
      "Human: whats the percentage of marketshare\n",
      "AI: As of 2020, China holds the largest market share in the global automotive industry at around 30%, followed by the United States with approximately 20%, Japan with around 12%, Germany with about 5%, and India with around 4%. These percentages can vary each year based on production levels and consumer demand.\n",
      "Human: whats the percentage of marketshare of cars\n",
      "AI: I'm sorry, I do not have specific data on the percentage of market share for individual countries in terms of cars specifically. However, based on the overall market share information I provided earlier, China is the largest producer of cars and likely holds a significant portion of the global market share for cars. Other countries like the United States, Japan, Germany, and India also contribute to the overall market share of cars.\n",
      "Human: which city of this country has more manufacturing units of cars\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'which city of this country has more manufacturing units of cars',\n",
       " 'history': 'Human: which contry is the largest producer of cars\\nAI: The largest producer of cars is currently China, followed by the United States, Japan, Germany, and India. China has the largest number of automobile manufacturers and produces the most cars globally. Japan is known for its high-quality vehicles and innovative technology. Germany is home to some of the world\\'s most prestigious car brands like Mercedes-Benz, BMW, and Volkswagen. India has a growing automotive industry, with companies like Tata Motors and Maruti Suzuki leading the way.\\nHuman: whats the percentage of cars\\nAI:  I\\'m sorry, could you please clarify what you mean by \"percentage of cars\"? Are you referring to market share, production volume, or something else?\\nHuman: whats the percentage of marketshare\\nAI: As of 2020, China holds the largest market share in the global automotive industry at around 30%, followed by the United States with approximately 20%, Japan with around 12%, Germany with about 5%, and India with around 4%. These percentages can vary each year based on production levels and consumer demand.\\nHuman: whats the percentage of marketshare of cars\\nAI: I\\'m sorry, I do not have specific data on the percentage of market share for individual countries in terms of cars specifically. However, based on the overall market share information I provided earlier, China is the largest producer of cars and likely holds a significant portion of the global market share for cars. Other countries like the United States, Japan, Germany, and India also contribute to the overall market share of cars.',\n",
       " 'response': \"In China, the city of Shanghai is considered a major hub for automobile manufacturing. Shanghai is home to a number of automotive companies and production facilities, contributing significantly to the country's overall car production. Other cities in China with a strong automotive industry include Beijing and Guangzhou. The Chinese government has invested heavily in developing the automotive sector in these cities to drive economic growth and innovation.\"}"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_chain.invoke({\"input\":\"which city of this country has more manufacturing units of cars\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "1ef9f3a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\somashekar.inguva\\AppData\\Local\\Temp\\ipykernel_34252\\4287167229.py:8: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferWindowMemory(k=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: which contry is the largest producer of Chacolates\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'which contry is the largest producer of Chacolates',\n",
       " 'history': '',\n",
       " 'response': \"The largest producer of chocolates in the world is the United States, followed by Germany and Switzerland. The United States produces over 35% of the world's chocolate, with famous companies like Hershey's and Mars dominating the market. Germany and Switzerland are known for their high-quality and luxury chocolates, with brands like Lindt and Toblerone being popular worldwide. These countries have a long history of chocolate production and are known for their delicious creations.\"}"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import PromptTemplate\n",
    "from langchain_openai import OpenAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain.chains import ConversationChain\n",
    "\n",
    "memory = ConversationBufferWindowMemory(k=1)\n",
    "\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "\n",
    "# chat_memory = ConversationBufferMemory(memory_key=\"history\",return_messages=True)\n",
    "\n",
    "Conv_chain = ConversationChain(llm = llm,memory=memory ,verbose = True )\n",
    "Conv_chain.invoke({\"input\":\"which contry is the largest producer of Chacolates\"})\n",
    "# chat_chain.invoke()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "ba2709a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: which contry is the largest producer of Chacolates\n",
      "AI: The largest producer of chocolates in the world is the United States, followed by Germany and Switzerland. The United States produces over 35% of the world's chocolate, with famous companies like Hershey's and Mars dominating the market. Germany and Switzerland are known for their high-quality and luxury chocolates, with brands like Lindt and Toblerone being popular worldwide. These countries have a long history of chocolate production and are known for their delicious creations.\n",
      "Human: which city of this contry is the largest producer of Chacolates\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'which city of this contry is the largest producer of Chacolates',\n",
       " 'history': \"Human: which contry is the largest producer of Chacolates\\nAI: The largest producer of chocolates in the world is the United States, followed by Germany and Switzerland. The United States produces over 35% of the world's chocolate, with famous companies like Hershey's and Mars dominating the market. Germany and Switzerland are known for their high-quality and luxury chocolates, with brands like Lindt and Toblerone being popular worldwide. These countries have a long history of chocolate production and are known for their delicious creations.\",\n",
       " 'response': 'The city in the United States that is the largest producer of chocolates is Hershey, Pennsylvania. Hershey is known as the \"Chocolate Capital of the World\" and is home to the Hershey Company, one of the largest chocolate manufacturers in the world. The city has a rich history of chocolate production dating back to the early 20th century when Milton Hershey established his chocolate factory there. Today, Hershey is a popular tourist destination with attractions like Hersheypark and the Hershey\\'s Chocolate World experience.'}"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Conv_chain.invoke({\"input\":\"which city of this contry is the largest producer of Chacolates\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "e249a9e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: which city of this contry is the largest producer of Chacolates\n",
      "AI: The city in the United States that is the largest producer of chocolates is Hershey, Pennsylvania. Hershey is known as the \"Chocolate Capital of the World\" and is home to the Hershey Company, one of the largest chocolate manufacturers in the world. The city has a rich history of chocolate production dating back to the early 20th century when Milton Hershey established his chocolate factory there. Today, Hershey is a popular tourist destination with attractions like Hersheypark and the Hershey's Chocolate World experience.\n"
     ]
    }
   ],
   "source": [
    "print(Conv_chain.memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "b6881806",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationSummaryMemory\n",
    "\n",
    "memory = ConversationSummaryMemory(llm=OpenAI())\n",
    "memory.save_context({\"input\": \"Which country is the biggest exporter of cotton?\"}, {\"output\": \"China\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "167d20dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': '\\nThe human asks the AI which country is the biggest exporter of cotton. The AI responds with China.'}"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "a6782746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: {'Which country is the biggest exporter of cotton?'}\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': {'Which country is the biggest exporter of cotton?'},\n",
       " 'history': '',\n",
       " 'response': ' The country that is currently the biggest exporter of cotton is India. In 2020, India exported over 6.2 billion US dollars worth of cotton, followed by the United States with 4.1 billion US dollars and Brazil with 2.3 billion US dollars. However, this data can vary depending on the source and year. For example, in 2019, the United States was the top exporter of cotton with over 5.9 billion US dollars worth of exports, followed by India with 5.8 billion US dollars and Brazil with 2.1 billion US dollars. Overall, India, the United States, and Brazil are consistently the top three exporters of cotton.'}"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = OpenAI()\n",
    "conv_with_summary = ConversationChain(\n",
    "    llm = llm,\n",
    "    memory = ConversationSummaryMemory(llm=OpenAI()),\n",
    "    verbose = True\n",
    ")\n",
    "conv_with_summary.invoke(input={\"Which country is the biggest exporter of cotton?\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "3a1dbd67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "The human inquires about the biggest exporter of cotton, and the AI responds that currently, India holds the top spot with over 6.2 billion US dollars worth of exports in 2020. However, this can fluctuate depending on the source and year. In 2019, the United States was the top exporter, followed closely by India and Brazil. Overall, these three countries are consistently the top exporters of cotton.\n",
      "Human: {'What is the total value of cotton exports of this country'}\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': {'What is the total value of cotton exports of this country'},\n",
       " 'history': '\\nThe human inquires about the biggest exporter of cotton, and the AI responds that currently, India holds the top spot with over 6.2 billion US dollars worth of exports in 2020. However, this can fluctuate depending on the source and year. In 2019, the United States was the top exporter, followed closely by India and Brazil. Overall, these three countries are consistently the top exporters of cotton.',\n",
       " 'response': 'That depends on which country you are referring to. In 2020, India was the biggest exporter of cotton with over 6.2 billion US dollars worth of exports. However, in 2019, the United States was the top exporter with over 6.5 billion US dollars worth of exports. In terms of overall value, the top three exporters of cotton are usually the United States, India, and Brazil.'}"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_with_summary.invoke(input={\"What is the total value of cotton exports of this country\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "ebdc006e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "The human asks about the total value of cotton exports for a specific country. The AI explains that this value changes depending on the source and year, but in 2020, India was the biggest exporter with over 6.2 billion US dollars worth of exports. However, in 2019, the United States was the top exporter with over 6.5 billion US dollars worth of exports. Generally, the top three exporters of cotton are consistently the United States, India, and Brazil.\n",
      "Human: {'What are the other contries followed by this country?'}\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': {'What are the other contries followed by this country?'},\n",
       " 'history': '\\nThe human asks about the total value of cotton exports for a specific country. The AI explains that this value changes depending on the source and year, but in 2020, India was the biggest exporter with over 6.2 billion US dollars worth of exports. However, in 2019, the United States was the top exporter with over 6.5 billion US dollars worth of exports. Generally, the top three exporters of cotton are consistently the United States, India, and Brazil.',\n",
       " 'response': ' After the top three exporters, the next countries in line for cotton exports are Pakistan, Australia, Uzbekistan, Turkey, and Greece. These countries have varying export values, with Pakistan usually being the fourth largest exporter and Australia being the fifth largest. However, this can change from year to year depending on various factors such as weather conditions and demand for cotton in the global market.'}"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_with_summary.invoke(input={\"What are the other contries followed by this country?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "aed2c2d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': '\\nThe human asks the AI which country is the biggest exporter of cotton. The AI responds with China.'}"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "7fa89198",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\somashekar.inguva\\AppData\\Local\\Temp\\ipykernel_34252\\3854045601.py:6: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  llm = ChatOpenAI(temperature=0)\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import ConversationSummaryMemory\n",
    "\n",
    "# Initialize the model\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "\n",
    "# Create memory that summarizes conversations\n",
    "memory = ConversationSummaryMemory(llm=llm)\n",
    "\n",
    "# Create the conversation chain with summary memory\n",
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    verbose=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "343d1723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hi, I want to book a hotel room in New York.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Bot: Hello! I can help you with that. What specific dates are you looking to book for?\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "The human wants to book a hotel room in New York. The AI offers to help and asks for specific dates for the booking.\n",
      "Human: From June 10th to June 15th.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Bot: Great! I can help you with that. What type of hotel are you looking for? Do you have a specific budget in mind?\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "The human wants to book a hotel room in New York from June 10th to June 15th. The AI offers to help and asks for specific dates for the booking, as well as the type of hotel and budget preferences.\n",
      "Human: I prefer a hotel with a gym.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Bot: Great choice! I can help you find a hotel with a gym in New York for your stay from June 10th to June 15th. Do you have a specific budget in mind for your hotel stay?\n"
     ]
    }
   ],
   "source": [
    "# User starts conversation\n",
    "response1 = conversation.predict(input=\"Hi, I want to book a hotel room in New York.\")\n",
    "print(\"Bot:\", response1)\n",
    "\n",
    "# User adds more details\n",
    "response2 = conversation.predict(input=\"From June 10th to June 15th.\")\n",
    "print(\"Bot:\", response2)\n",
    "\n",
    "# User adds a preference\n",
    "response3 = conversation.predict(input=\"I prefer a hotel with a gym.\")\n",
    "print(\"Bot:\", response3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "f1a8b3fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'The human wants to book a hotel room in New York from June 10th to June 15th. The AI offers to help and asks for specific dates for the booking, as well as the type of hotel and budget preferences. The human prefers a hotel with a gym, and the AI assures them they can find a hotel with a gym in New York for their stay from June 10th to June 15th, asking about their budget for the booking.'}"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "310c44bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "The human wants to book a hotel room in New York from June 10th to June 15th. The AI offers to help and asks for specific dates for the booking, as well as the type of hotel and budget preferences. The human prefers a hotel with a gym, and the AI assures them they can find a hotel with a gym in New York for their stay from June 10th to June 15th, asking about their budget for the booking.\n",
      "Human: I prefer a hotel with a beachview also.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Bot: I'm sorry, but New York City does not have any hotels with beach views as it is not located on the coast. However, I can still help you find a hotel with a gym within your budget for your stay from June 10th to June 15th. Can you please provide me with your budget range for the booking?\n"
     ]
    }
   ],
   "source": [
    "response4 = conversation.predict(input=\"I prefer a hotel with a beachview also.\")\n",
    "print(\"Bot:\", response4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "10cf80ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'The human wants to book a hotel room in New York from June 10th to June 15th. The AI offers to help and asks for specific dates for the booking, as well as the type of hotel and budget preferences. The human prefers a hotel with a gym and a beach view, but the AI explains that New York City does not have any hotels with beach views. Nonetheless, the AI assures the human that they can find a hotel with a gym within their budget for their stay from June 10th to June 15th, asking about their budget range for the booking.'}"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "df545df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "model = ChatOpenAI()\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are having a chat with a human.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "chain = prompt | model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "36daaf30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pypdf in c:\\users\\somashekar.inguva\\onedrive - telstra\\documents\\genai_projects\\venv_genai\\lib\\site-packages (5.4.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "228faf6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool,BaseTool, StructuredTool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f25803b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def name_of_tool(input:str):\n",
    "    \"\"\"Tool_Description\"\"\"\n",
    "    return \"result\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74addc9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name_of_tool\n",
      "Tool_Description\n",
      "{'description': 'Tool_Description', 'properties': {'input': {'title': 'Input', 'type': 'string'}}, 'required': ['input'], 'title': 'name_of_tool', 'type': 'object'}\n"
     ]
    }
   ],
   "source": [
    "print(name_of_tool.name)\n",
    "print(name_of_tool.description)\n",
    "print(name_of_tool.args_schema.model_json_schema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "f0d8b535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mSignature:\u001b[39m      name_of_tool(tool_input: \u001b[33m'str'\u001b[39m, callbacks: \u001b[33m'Callbacks'\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m) -> \u001b[33m'str'\u001b[39m\n",
      "\u001b[31mType:\u001b[39m           StructuredTool\n",
      "\u001b[31mString form:\u001b[39m    name='name_of_tool' description='Tool_Description' args_schema=<class 'langchain_core.utils.pydantic.name_of_tool'> func=<function name_of_tool at 0x000002208A1A8C20>\n",
      "\u001b[31mFile:\u001b[39m           c:\\users\\somashekar.inguva\\onedrive - telstra\\documents\\genai_projects\\venv_genai\\lib\\site-packages\\langchain_core\\tools\\structured.py\n",
      "\u001b[31mDocstring:\u001b[39m      Tool that can operate on any number of inputs.\n",
      "\u001b[31mInit docstring:\u001b[39m Initialize the tool.\n",
      "\u001b[31mCall docstring:\u001b[39m\n",
      ".. deprecated:: 0.1.47 Use :meth:`~invoke` instead. It will not be removed until langchain-core==1.0.\n",
      "\n",
      "Make tool callable."
     ]
    }
   ],
   "source": [
    "name_of_tool?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77915124",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def sta_coins(input: float)->float:\n",
    "    \"\"\" Use this tool to convert USD to Start-Tech Academy coins\"\"\"\n",
    "    return 1.3*(float(input))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18741e06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use this tool to convert USD to Start-Tech Academy coins\n",
      "{'input': {'title': 'Input', 'type': 'number'}}\n"
     ]
    }
   ],
   "source": [
    "print(sta_coins.description)\n",
    "print(sta_coins.args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8fb7962d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "\n",
    "llm_bind_tools = llm.bind_tools([sta_coins])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "61003e3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_Z1yYehWz1AzjRUMw6gNj0LtW', 'function': {'arguments': '{\"input\":10}', 'name': 'sta_coins'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 16, 'prompt_tokens': 64, 'total_tokens': 80, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BX12ucN4nXPsFLMXWTFE9fp3aD9hx', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-668cbff6-306e-4450-bc15-d5a07909a3ab-0', tool_calls=[{'name': 'sta_coins', 'args': {'input': 10}, 'id': 'call_Z1yYehWz1AzjRUMw6gNj0LtW', 'type': 'tool_call'}], usage_metadata={'input_tokens': 64, 'output_tokens': 16, 'total_tokens': 80, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = llm_bind_tools.invoke(\"How many start-tech academy coins can I get for USD10\")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ed01b202",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'sta_coins',\n",
       "  'args': {'input': 10},\n",
       "  'id': 'call_Z1yYehWz1AzjRUMw6gNj0LtW',\n",
       "  'type': 'tool_call'}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# type(result)\n",
    "result.tool_calls[0]['name']\n",
    "result.tool_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c83d88e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sta_coins': StructuredTool(name='sta_coins', description='Use this tool to convert USD to Start-Tech Academy coins', args_schema=<class 'langchain_core.utils.pydantic.sta_coins'>, func=<function sta_coins at 0x000002C0AD6EC540>)}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_mapping = {\n",
    "    'sta_coins' : sta_coins\n",
    "}\n",
    "tool_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4f8ac37b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tool_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a0009e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "tool = tool_mapping['sta_coins']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f7da3dae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_op = tool.invoke(result.tool_calls[0]['args'])\n",
    "tool_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b32aa34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet  duckduckgo-search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9844359c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "# from langchain_community.tools import DuckDuckGoSearchRun\n",
    "search = DuckDuckGoSearchRun()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b36aeeb5",
   "metadata": {},
   "outputs": [
    {
     "ename": "DuckDuckGoSearchException",
     "evalue": "https://html.duckduckgo.com/html 202 Ratelimit",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mDuckDuckGoSearchException\u001b[39m                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43msearch\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mwhat is Langchain\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\somashekar.inguva\\OneDrive - Telstra\\Documents\\GenAI_Projects\\venv_GenAI\\Lib\\site-packages\\langchain_core\\tools\\base.py:513\u001b[39m, in \u001b[36mBaseTool.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    505\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    506\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    507\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    510\u001b[39m     **kwargs: Any,\n\u001b[32m    511\u001b[39m ) -> Any:\n\u001b[32m    512\u001b[39m     tool_input, kwargs = _prep_run_args(\u001b[38;5;28minput\u001b[39m, config, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m513\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtool_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\somashekar.inguva\\OneDrive - Telstra\\Documents\\GenAI_Projects\\venv_GenAI\\Lib\\site-packages\\langchain_core\\tools\\base.py:774\u001b[39m, in \u001b[36mBaseTool.run\u001b[39m\u001b[34m(self, tool_input, verbose, start_color, color, callbacks, tags, metadata, run_name, run_id, config, tool_call_id, **kwargs)\u001b[39m\n\u001b[32m    772\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m error_to_raise:\n\u001b[32m    773\u001b[39m     run_manager.on_tool_error(error_to_raise)\n\u001b[32m--> \u001b[39m\u001b[32m774\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m error_to_raise\n\u001b[32m    775\u001b[39m output = _format_output(content, artifact, tool_call_id, \u001b[38;5;28mself\u001b[39m.name, status)\n\u001b[32m    776\u001b[39m run_manager.on_tool_end(output, color=color, name=\u001b[38;5;28mself\u001b[39m.name, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\somashekar.inguva\\OneDrive - Telstra\\Documents\\GenAI_Projects\\venv_GenAI\\Lib\\site-packages\\langchain_core\\tools\\base.py:743\u001b[39m, in \u001b[36mBaseTool.run\u001b[39m\u001b[34m(self, tool_input, verbose, start_color, color, callbacks, tags, metadata, run_name, run_id, config, tool_call_id, **kwargs)\u001b[39m\n\u001b[32m    741\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m config_param := _get_runnable_config_param(\u001b[38;5;28mself\u001b[39m._run):\n\u001b[32m    742\u001b[39m         tool_kwargs = tool_kwargs | {config_param: config}\n\u001b[32m--> \u001b[39m\u001b[32m743\u001b[39m     response = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mtool_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mtool_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.response_format == \u001b[33m\"\u001b[39m\u001b[33mcontent_and_artifact\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    745\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(response) != \u001b[32m2\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\somashekar.inguva\\OneDrive - Telstra\\Documents\\GenAI_Projects\\venv_GenAI\\Lib\\site-packages\\langchain_community\\tools\\ddg_search\\tool.py:74\u001b[39m, in \u001b[36mDuckDuckGoSearchRun._run\u001b[39m\u001b[34m(self, query, run_manager)\u001b[39m\n\u001b[32m     68\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_run\u001b[39m(\n\u001b[32m     69\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m     70\u001b[39m     query: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m     71\u001b[39m     run_manager: Optional[CallbackManagerForToolRun] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m     72\u001b[39m ) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m     73\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Use the tool.\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m74\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapi_wrapper\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\somashekar.inguva\\OneDrive - Telstra\\Documents\\GenAI_Projects\\venv_GenAI\\Lib\\site-packages\\langchain_community\\utilities\\duckduckgo_search.py:114\u001b[39m, in \u001b[36mDuckDuckGoSearchAPIWrapper.run\u001b[39m\u001b[34m(self, query)\u001b[39m\n\u001b[32m    112\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Run query through DuckDuckGo and return concatenated results.\"\"\"\u001b[39;00m\n\u001b[32m    113\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.source == \u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m     results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_ddgs_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    115\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.source == \u001b[33m\"\u001b[39m\u001b[33mnews\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    116\u001b[39m     results = \u001b[38;5;28mself\u001b[39m._ddgs_news(query)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\somashekar.inguva\\OneDrive - Telstra\\Documents\\GenAI_Projects\\venv_GenAI\\Lib\\site-packages\\langchain_community\\utilities\\duckduckgo_search.py:64\u001b[39m, in \u001b[36mDuckDuckGoSearchAPIWrapper._ddgs_text\u001b[39m\u001b[34m(self, query, max_results)\u001b[39m\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mduckduckgo_search\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DDGS\n\u001b[32m     63\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m DDGS() \u001b[38;5;28;01mas\u001b[39;00m ddgs:\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m     ddgs_gen = \u001b[43mddgs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[43m        \u001b[49m\u001b[43mregion\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mregion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m     67\u001b[39m \u001b[43m        \u001b[49m\u001b[43msafesearch\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msafesearch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimelimit\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtime\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_results\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_results\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_results\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     72\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ddgs_gen:\n\u001b[32m     73\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m [r \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m ddgs_gen]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\somashekar.inguva\\OneDrive - Telstra\\Documents\\GenAI_Projects\\venv_GenAI\\Lib\\site-packages\\duckduckgo_search\\duckduckgo_search.py:184\u001b[39m, in \u001b[36mDDGS.text\u001b[39m\u001b[34m(self, keywords, region, safesearch, timelimit, backend, max_results)\u001b[39m\n\u001b[32m    181\u001b[39m         logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError to search using \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mb\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m backend: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mex\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    182\u001b[39m         err = ex\n\u001b[32m--> \u001b[39m\u001b[32m184\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m DuckDuckGoSearchException(err)\n",
      "\u001b[31mDuckDuckGoSearchException\u001b[39m: https://html.duckduckgo.com/html 202 Ratelimit"
     ]
    }
   ],
   "source": [
    "search.invoke(\"what is Langchain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "089006c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  DEPRECATION: wikipedia is being installed using the legacy 'setup.py install' method, because it does not have a 'pyproject.toml' and the 'wheel' package is not installed. pip 23.1 will enforce this behaviour change. A possible replacement is to enable the '--use-pep517' option. Discussion can be found at https://github.com/pypa/pip/issues/8559\n",
      "\n",
      "[notice] A new release of pip available: 22.3 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet  wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "22467e7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Page: Retrieval-augmented generation\\nSummary: Retrieval-augmented generation (RAG) is a technique that enables generative artificial intelligence (Gen AI) models to retrieve and incorporate new information. It modifies interactions with a large language model (LLM) so that the model responds to user queries with reference to a specified set of documents, using this information to supplement information from its pre-existing training data. This allows LLMs to use domain-specific and/or updated information. Use cases include providing chatbot access to internal company data or generating responses based on authoritative sources.\\nRAG improves large language models (LLMs) by incorporating information retrieval before generating responses. Unlike traditional LLMs that rely on static training data, RAG pulls relevant text from databases, uploaded documents, or web sources. According to Ars Technica, \"RAG is a way of improving LLM performance, in essence by blending the LLM process with a web search or other document look-up process to help LLMs stick to the facts.\" This method helps reduce AI hallucinations, which have led to real-world issues like chatbots inventing policies or lawyers citing nonexistent legal cases.\\nBy dynamically retrieving information, RAG enables AI to provide more accurate responses without frequent retraining. According to IBM, \"RAG also reduces the need for users to continuously train the model on new data and update its parameters as circumstances evolve. In this way, RAG can lower the computational and financial costs of running LLM-powered chatbots in an enterprise setting.\"\\nBeyond efficiency gains, RAG also allows LLMs to include source references in their responses, enabling users to verify information by reviewing cited documents or original sources. This can provide greater transparency, as users can cross-check retrieved content to ensure accuracy and relevance.\\nThe term RAG was first introduced in 2020 by Douwe Kiela, Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, and Sebastian Riedel in their research paper Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks, at Meta.\\n\\nPage: Intelligent agent\\nSummary: In artificial intelligence, an intelligent agent is an entity that perceives its environment, takes actions autonomously to achieve goals, and may improve its performance through machine learning or by acquiring knowledge. Leading AI textbooks define artificial intelligence as the \"study and design of intelligent agents,\" emphasizing that goal-directed behavior is central to intelligence.\\nA specialized subset of intelligent agents, agentic AI (also known as an AI agent or simply agent), expands this concept by proactively pursuing goals, making decisions, and taking actions over extended periods, thereby exemplifying a novel form of digital agency.\\nIntelligent agents can range from simple to highly complex. A basic thermostat or control system is considered an intelligent agent, as is a human being, or any other system that meets the same criteria—such as a firm, a state, or a biome.\\nIntelligent agents operate based on an objective function, which encapsulates their goals. They are designed to create and execute plans that maximize the expected value of this function upon completion. For example, a reinforcement learning agent has a reward function, which allows programmers to shape its desired behavior. Similarly, an evolutionary algorithm\\'s behavior is guided by a fitness function.\\nIntelligent agents in artificial intelligence are closely related to agents in economics, and versions of the intelligent agent paradigm are studied in cognitive science, ethics, and the philosophy of practical reason, as well as in many interdisciplinary socio-cognitive modeling and computer social simulations.\\nIntelligent agents are often described schematically as abstract functional systems similar '"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.tools import WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper\n",
    "\n",
    "wikidata = WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper())\n",
    "\n",
    "wikidata.invoke(\"what is langchain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e80d30b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentExecutor,create_tool_calling_agent\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "17bf6408",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=1)\n",
    "# tools = [search, wikidata]\n",
    "tools = [wikidata]\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant. For answering the user query, look for information using \"\n",
    "            \" Wikipedia and then give the final answer\",\n",
    "        ),\n",
    "        (\"placeholder\", \"{chat_history}\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "        (\"placeholder\", \"{agent_scratchpad}\"),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120271bc",
   "metadata": {},
   "source": [
    "| Term               | Description                                                                 |\n",
    "| ------------------ | --------------------------------------------------------------------------- |\n",
    "| `agent_scratchpad` | A formatted log of intermediate agent reasoning steps and tool interactions |\n",
    "| Purpose            | Helps the agent stay coherent and avoid repeating past steps                |\n",
    "| Used in            | Prompt templates inside multi-step agent loops                              |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "688c852e",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = create_tool_calling_agent(llm=llm,tools=tools, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "978358cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `wikipedia` with `{'query': 'universe'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mPage: Universe\n",
      "Summary: The universe is all of space and time and their contents. It comprises all of existence, any fundamental interaction, physical process and physical constant, and therefore all forms of matter and energy, and the structures they form, from sub-atomic particles to entire galactic filaments. Since the early 20th century, the field of cosmology establishes that space and time emerged together at the Big Bang 13.787±0.020 billion years ago and that the universe has been expanding since then. The portion of the universe that can be seen by humans is approximately 93 billion light-years in diameter at present, but the total size of the universe is not known.\n",
      "Some of the earliest cosmological models of the universe were developed by ancient Greek and Indian philosophers and were geocentric, placing Earth at the center. Over the centuries, more precise astronomical observations led Nicolaus Copernicus to develop the heliocentric model with the Sun at the center of the Solar System. In developing the law of universal gravitation, Isaac Newton built upon Copernicus's work as well as Johannes Kepler's laws of planetary motion and observations by Tycho Brahe.\n",
      "Further observational improvements led to the realization that the Sun is one of a few hundred billion stars in the Milky Way, which is one of a few hundred billion galaxies in the observable universe. Many of the stars in a galaxy have planets. At the largest scale, galaxies are distributed uniformly and the same in all directions, meaning that the universe has neither an edge nor a center. At smaller scales, galaxies are distributed in clusters and superclusters which form immense filaments and voids in space, creating a vast foam-like structure. Discoveries in the early 20th century have suggested that the universe had a beginning and has been expanding since then.\n",
      "According to the Big Bang theory, the energy and matter initially present have become less dense as the universe expanded. After an initial accelerated expansion called the inflationary epoch at around 10−32 seconds, and the separation of the four known fundamental forces, the universe gradually cooled and continued to expand, allowing the first subatomic particles and simple atoms to form. Giant clouds of hydrogen and helium were gradually drawn to the places where matter was most dense, forming the first galaxies, stars, and everything else seen today.\n",
      "From studying the effects of gravity on both matter and light, it has been discovered that the universe contains much more matter than is accounted for by visible objects; stars, galaxies, nebulas and interstellar gas. This unseen matter is known as dark matter. In the widely accepted ΛCDM cosmological model, dark matter accounts for about 25.8%±1.1% of the mass and energy in the universe while about 69.2%±1.2% is dark energy, a mysterious form of energy responsible for the acceleration of the expansion of the universe. Ordinary ('baryonic') matter therefore composes only 4.84%±0.1% of the universe. Stars, planets, and visible gas clouds only form about 6% of this ordinary matter.\n",
      "There are many competing hypotheses about the ultimate fate of the universe and about what, if anything, preceded the Big Bang, while other physicists and philosophers refuse to speculate, doubting that information about prior states will ever be accessible. Some physicists have suggested various multiverse hypotheses, in which the universe might be one among many.\n",
      "\n",
      "Page: Miss Universe\n",
      "Summary: Miss Universe is an annual international major beauty pageant that is run by a Thailand and Mexican-based Miss Universe Organization. Along with Miss World, Miss International, and Miss Earth, it is one of the Big Four beauty pageants.\n",
      "The Miss Universe Organization and its brand, is currently owned by JKN Global Group and Legacy Holding Group USA Inc., an American division of Mexican company Legacy Holding through the joint venture company JKN Legacy Inc. Telemundo has the licens\u001b[0m\u001b[32;1m\u001b[1;3mA fun fact about the universe is that the universe contains much more matter than is accounted for by visible objects like stars, galaxies, nebulas, and interstellar gas. This unseen matter is known as dark matter and accounts for about 25.8% of the mass and energy in the universe, while about 69.2% is dark energy, a mysterious form of energy responsible for the acceleration of the expansion of the universe. Ordinary ('baryonic') matter, which includes stars, planets, and visible gas clouds, only forms about 4.84% of the universe.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'fun fact about universe?',\n",
       " 'output': \"A fun fact about the universe is that the universe contains much more matter than is accounted for by visible objects like stars, galaxies, nebulas, and interstellar gas. This unseen matter is known as dark matter and accounts for about 25.8% of the mass and energy in the universe, while about 69.2% is dark energy, a mysterious form of energy responsible for the acceleration of the expansion of the universe. Ordinary ('baryonic') matter, which includes stars, planets, and visible gas clouds, only forms about 4.84% of the universe.\"}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor = AgentExecutor(agent=agent,tools=tools,verbose=True)\n",
    "agent_executor.invoke({\"input\":\"fun fact about universe?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e5f974d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant. For answering the user query, look for information using \"\n",
    "            \" Wikipedia and then give the final answer\",\n",
    "        ),\n",
    "        (\"placeholder\", \"{history}\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "        (\"placeholder\", \"{agent_scratchpad}\"),\n",
    "    ]\n",
    ")\n",
    "agent = create_tool_calling_agent(llm,tools,prompt)\n",
    "agent_executor = AgentExecutor(agent=agent,tools=tools,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f5de555f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a9e18e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "store = {}\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "62a0fe9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "agent_with_hist = RunnableWithMessageHistory(\n",
    "    agent_executor,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"history\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1c61abb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `wikipedia` with `{'query': 'Narendra Modi'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mPage: Narendra Modi\n",
      "Summary: Narendra Damodardas Modi (born 17 September 1950) is an Indian politician who has served as the  prime minister of India since 2014. Modi was the chief minister of Gujarat from 2001 to 2014 and is the member of parliament (MP) for Varanasi. He is a member of the Bharatiya Janata Party (BJP) and of the Rashtriya Swayamsevak Sangh (RSS), a right-wing Hindu nationalist paramilitary volunteer organisation. He is the longest-serving prime minister outside the Indian National Congress.\n",
      "Modi was born and raised in Vadnagar, Gujarat, where he completed his secondary education. He was introduced to the RSS at the age of eight. Modi became a full-time worker for the RSS in Gujarat in 1971. The RSS assigned him to the BJP in 1985 and he rose through the party hierarchy, becoming general secretary in 1998. In 2001, Modi was appointed chief minister of Gujarat and elected to the legislative assembly soon after. His administration is considered complicit in the 2002 Gujarat riots, and has been criticised for its management of the crisis. According to official records, a little over 1,000 people were killed, three-quarters of whom were Muslim; independent sources estimated 2,000 deaths, mostly Muslim. A Special Investigation Team appointed by the Supreme Court of India in 2012 found no evidence to initiate prosecution proceedings against him. While his policies as chief minister were credited for encouraging economic growth, his administration was criticised for failing to significantly improve health, poverty and education indices in the state.\n",
      "In the 2014 Indian general election, Modi led the BJP to a parliamentary majority, the first for a party since 1984. His administration increased direct foreign investment, and reduced spending on healthcare, education, and social-welfare programmes. Modi began a high-profile sanitation campaign, and weakened or abolished environmental and labour laws. His demonetisation of banknotes in 2016 and introduction of the Goods and Services Tax in 2017 sparked controversy. Modi's administration launched the 2019 Balakot airstrike against an alleged terrorist training camp in Pakistan. The airstrike failed, but the action had nationalist appeal. Modi's party won the 2019 general election which followed. In its second term, his administration revoked the special status of Jammu and Kashmir, and introduced the Citizenship Amendment Act, prompting widespread protests, and spurring the 2020 Delhi riots in which Muslims were brutalised and killed by Hindu mobs. Three controversial farm laws led to sit-ins by farmers across the country, eventually causing their formal repeal. Modi oversaw India's response to the COVID-19 pandemic, during which, according to the World Health Organization's estimates, 4.7 million Indians died. In the 2024 general election, Modi's party lost its majority in the lower house of Parliament and formed a government leading the National Democratic Alliance coalition. Modi presided over the 2025 India–Pakistan conflict, which resulted in a ceasefire.\n",
      "Under Modi's tenure, India has experienced democratic backsliding and shifted towards an authoritarian style of government with a personality cult. As prime minister, he has received consistently high approval ratings within India. Modi has been described as engineering a political realignment towards right-wing politics. He remains a highly controversial figure domestically and internationally, over his Hindu nationalist beliefs and handling of the Gujarat riots, which have been cited as evidence of a majoritarian and exclusionary social agenda.\n",
      "\n",
      "Page: Narendra Modi Stadium\n",
      "Summary: The Narendra Modi Stadium is an international cricket stadium located in Ahmedabad, Gujarat, a state in west India. With a capacity of 132,000 (total), 114,600 (seated) it is the world's largest stadium. The stadium, owned by the Gujarat Cricket Association, hosts both domestic and international cricket matches. It is recognized as one \u001b[0m\u001b[32;1m\u001b[1;3mNarendra Modi was born on September 17, 1950.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'when was Narendra modi born',\n",
       " 'history': [],\n",
       " 'output': 'Narendra Modi was born on September 17, 1950.'}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_with_hist.invoke({\"input\":\"when was Narendra modi born\"},config={\"configurable\":{\"session_id\":\"sess1\"}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "70d22454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `wikipedia` with `{'query': 'Narendra Modi'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mPage: Narendra Modi\n",
      "Summary: Narendra Damodardas Modi (born 17 September 1950) is an Indian politician who has served as the  prime minister of India since 2014. Modi was the chief minister of Gujarat from 2001 to 2014 and is the member of parliament (MP) for Varanasi. He is a member of the Bharatiya Janata Party (BJP) and of the Rashtriya Swayamsevak Sangh (RSS), a right-wing Hindu nationalist paramilitary volunteer organisation. He is the longest-serving prime minister outside the Indian National Congress.\n",
      "Modi was born and raised in Vadnagar, Gujarat, where he completed his secondary education. He was introduced to the RSS at the age of eight. Modi became a full-time worker for the RSS in Gujarat in 1971. The RSS assigned him to the BJP in 1985 and he rose through the party hierarchy, becoming general secretary in 1998. In 2001, Modi was appointed chief minister of Gujarat and elected to the legislative assembly soon after. His administration is considered complicit in the 2002 Gujarat riots, and has been criticised for its management of the crisis. According to official records, a little over 1,000 people were killed, three-quarters of whom were Muslim; independent sources estimated 2,000 deaths, mostly Muslim. A Special Investigation Team appointed by the Supreme Court of India in 2012 found no evidence to initiate prosecution proceedings against him. While his policies as chief minister were credited for encouraging economic growth, his administration was criticised for failing to significantly improve health, poverty and education indices in the state.\n",
      "In the 2014 Indian general election, Modi led the BJP to a parliamentary majority, the first for a party since 1984. His administration increased direct foreign investment, and reduced spending on healthcare, education, and social-welfare programmes. Modi began a high-profile sanitation campaign, and weakened or abolished environmental and labour laws. His demonetisation of banknotes in 2016 and introduction of the Goods and Services Tax in 2017 sparked controversy. Modi's administration launched the 2019 Balakot airstrike against an alleged terrorist training camp in Pakistan. The airstrike failed, but the action had nationalist appeal. Modi's party won the 2019 general election which followed. In its second term, his administration revoked the special status of Jammu and Kashmir, and introduced the Citizenship Amendment Act, prompting widespread protests, and spurring the 2020 Delhi riots in which Muslims were brutalised and killed by Hindu mobs. Three controversial farm laws led to sit-ins by farmers across the country, eventually causing their formal repeal. Modi oversaw India's response to the COVID-19 pandemic, during which, according to the World Health Organization's estimates, 4.7 million Indians died. In the 2024 general election, Modi's party lost its majority in the lower house of Parliament and formed a government leading the National Democratic Alliance coalition. Modi presided over the 2025 India–Pakistan conflict, which resulted in a ceasefire.\n",
      "Under Modi's tenure, India has experienced democratic backsliding and shifted towards an authoritarian style of government with a personality cult. As prime minister, he has received consistently high approval ratings within India. Modi has been described as engineering a political realignment towards right-wing politics. He remains a highly controversial figure domestically and internationally, over his Hindu nationalist beliefs and handling of the Gujarat riots, which have been cited as evidence of a majoritarian and exclusionary social agenda.\n",
      "\n",
      "Page: Narendra Modi Stadium\n",
      "Summary: The Narendra Modi Stadium is an international cricket stadium located in Ahmedabad, Gujarat, a state in west India. With a capacity of 132,000 (total), 114,600 (seated) it is the world's largest stadium. The stadium, owned by the Gujarat Cricket Association, hosts both domestic and international cricket matches. It is recognized as one \u001b[0m\u001b[32;1m\u001b[1;3mNarendra Modi became the Prime Minister of India for the first time after the 2014 Indian general election.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "result = agent_with_hist.invoke({\"input\":\"When he became the PM for the firsttime\"},config={\"configurable\":{\"session_id\":\"sess1\"}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f81264",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Narendra Modi became the Prime Minister of India for the first time after the 2014 Indian general election.'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "result['output']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_GenAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
